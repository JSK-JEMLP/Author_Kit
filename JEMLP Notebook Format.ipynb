{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fbf9b2",
   "metadata": {},
   "source": [
    "<small>Journal of Efficient Machine Learning Practice, Vol. 1 (2022)</small><br>\n",
    "<small>Submitted 1/22; Published 2/22</small>\n",
    "\n",
    "<h1><center>Journal of Efficient Machine Learning Practice<br>Notebook Format</center></h1>\n",
    "\n",
    "<b>Jonathan S. Kent</b>, jonathan.s.kent@lmco.com<br>\n",
    "*Advanced Technology Center*<br>\n",
    "*Lockheed Martin*<br>\n",
    "*Sunnyvale, CA 94089, USA*<br>\n",
    "\n",
    "<b>Jonathan S. Kent</b>, jskent2@illinois.edu<br>\n",
    "*Department of Mathematics*<br>\n",
    "*University of Illinois at Urbana-Champaign*<br>\n",
    "*Urbana, IL 61081, USA*<br>\n",
    "\n",
    "<b>Editor: </b>Jonathan S. Kent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bea953",
   "metadata": {},
   "source": [
    "<h2>README</h2>\n",
    "    \n",
    "Welcome to the Journal of Efficient Machine Learning Practice. This is a journal that will be focusing on efficiency, robustness, and real-world application of Machine Learning, and also to promote the writing of reader-friendly articles. If that seems up your alley, you are welcome to make an account and submit a paper to the [Journal of Efficient Machine Learning Practice](https:\\\\www.jemlp.org).\n",
    "\n",
    "The purpose of the README, here, is similar to that of the README of a given software package. It should explain in broad strokes how your code functions, and what considerations a reader should have going into it, e.g. hardware and software, such as \"This code was written for a machine running Ubuntu 22.04, with an RTX 3070, a Ryzen 5 3600, and 16 GB of RAM.\"\n",
    "\n",
    "Also, do not take the structure of this example notebook as gospel. If your code is cleaner and more sensible when organized in some other way, then structure it as is appropriate. However, your notebook should be structured so that selecting \"Kernel -> Restart & Run All\" completes one entire experimental run, and produces graphs and outputs as they would appear in your manuscript, e.g. accuracy metrics, training loss over time, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short explanations, like \"importing packages,\" should be given at the start of code cells with an\n",
    "# obvious purpose.\n",
    "\n",
    "# A requirements .yml file listing packages and versions in standard Anaconda format must be included\n",
    "# with your submission\n",
    "\n",
    "# This .yml file is produced by running \n",
    "# `conda env export --from-history>ENV.yml`\n",
    "# and can be install by the reader by running\n",
    "# `conda env create -n ENVNAME --file ENV.yml`\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import progressbar\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ac099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and environmental constants should be given immediately after the import cells,\n",
    "# and should have clear, readable names. If, in the context of reading the manuscript, the name alone\n",
    "# is not enough to explain what the hyperparameter is, a short explanatory comment should be given. \n",
    "# Feel free to directly reference the manuscript that will be submitted alongside this notebook.\n",
    "# Being constants, hyperparameters should be in block capital snake case. For the sake of\n",
    "# consistency, even if something listed here is a function, e.g. a choice of loss function,\n",
    "# it should still be in block capital snake case, to represent that it was just something arbitrarily\n",
    "# chosen.\n",
    "\n",
    "# Avoid leaving configuration-specific environmental constants, like directory names, in the submitted\n",
    "# version of the notebook\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "DATA_LOC = '/path/to/dataset'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEPTH = 5 # Number of convolutional layers in the CNN\n",
    "CHANNELS = 32 # Number of channels in the convolutional layers\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "TRAINING_TRANSFORMS = torchvision.transforms.Compose([ # Transforms used during model training\n",
    "    torchvision.transforms.Resize([32, 32]),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(10),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "TESTING_TRANSFORMS = torchvision.transforms.Compose([ # Transforms used during testing\n",
    "    torchvision.transforms.Resize([32, 32]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302eba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters and environmental constants that are immediately derived from those in the\n",
    "# preceeding cells should be defined immediately after them, and should be written with all lowercase\n",
    "# snake case\n",
    "\n",
    "training_dataset = torchvision.datasets.MNIST(DATA_LOC, train = True, download = True, \n",
    "                                                 transform = TRAINING_TRANSFORMS)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size = BATCH_SIZE, shuffle = True,\n",
    "                                                 pin_memory = True, num_workers = 4)\n",
    "testing_dataset = torchvision.datasets.MNIST(DATA_LOC, train = False, download = True,\n",
    "                                                 transform = TESTING_TRANSFORMS)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f649794",
   "metadata": {},
   "source": [
    "<h2>1. Model</h2>\n",
    "\n",
    "Section headers should have a number associated with them, as well as a title, for easy reference. Feel free to include longer-form commentary on what you're about to do in the next few cells, if you think that it will help to explain things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e61a6b",
   "metadata": {},
   "source": [
    "<h3>1.1. Model Definition</h3>\n",
    "\n",
    "Section and subsection headers should be written according to the following example:\n",
    "```\n",
    "<h2>3. Section name</h2>\n",
    "<h3>3.1. Subsection name</h3>\n",
    "<h4>3.1.4. Subsubsection name</h4>\n",
    "```\n",
    "etc. This will make it easy to reference these sections in the manuscript, e.g. \"This algorithm, which is implemented in notebook section X.Y.Z.\" New sections and subsections should be used to divide up your code into segments that are relevant and related within themselves, and should be ordered in a coherent, sensible fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the classifier model\n",
    "\n",
    "# Class names should be in capital camel case, while both functions and variables should be in lower snake case\n",
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Creating the convolutional layers\n",
    "        cnn_layers = [torch.nn.Conv2d(1, CHANNELS, 3, padding = 1, bias = False)]\n",
    "        for i in range(1, DEPTH):\n",
    "            cnn_layers += [torch.nn.BatchNorm2d(CHANNELS), torch.nn.ReLU(), \n",
    "                           torch.nn.Conv2d(CHANNELS, CHANNELS, 3, padding = 1, bias = False)]\n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        # Averaging over each channel after the convolutional layers, and a fully connected layer\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn = torch.nn.Linear(channels, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        z = self.pool(z)\n",
    "        z = torch.flatten(z, 1) # Flattening a dimension so that the fully connected layer can take z as input\n",
    "        return(self.fcn(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d55e9",
   "metadata": {},
   "source": [
    "<h2>2. Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and training the model\n",
    "model = ClassifierModel().to(DEVICE)\n",
    "\n",
    "# Setting up the optimizer and gradscaler, to make use of FP16 hardware on modern Nvidia GPUs\n",
    "opt = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for e in progressbar.progressbar(range(NUM_EPOCHS)):\n",
    "    for i, (x_, y_) in enumerate(training_dataloader):\n",
    "        x, y = x_.to(DEVICE), y_.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_hat = model(x)\n",
    "            loss = LOSS_FN(y_hat, y)\n",
    "\n",
    "        # Using the gradscaler on the loss value and the optimizer\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Graphing the training loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss per step\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss (cross entropy)\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c81cbe",
   "metadata": {},
   "source": [
    "<h2>3. Testing</h2>\n",
    "\n",
    "In total, your notebook should be relatively light on text; that's what the manuscript is for. However, please try and leave an appropriate number of comments, as well as explaining any discrepancies between your implementation and your Mathematical formulation, as well as any interesting practical considerations you made during implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a00508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the testing set\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, (x_, y_) in enumerate(testing_dataloader):\n",
    "    # Performing inference without holding gradients\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        y_hat = model(x_.to(DEVICE)).to('cpu')\n",
    "    \n",
    "    total += len(y_)\n",
    "    correct += (y_hat.argmax(axis = 1) == y_).to(torch.float32).sum().item()\n",
    "    \n",
    "print(\"Accuracy:\", correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
